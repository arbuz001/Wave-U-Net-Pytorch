{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962ddc1-9ee5-4e23-afba-4baa08490c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model.utils as model_utils\n",
    "import utils\n",
    "from data.dataset import SeparationDataset\n",
    "from data.musdb import get_musdb_folds\n",
    "from data.utils import crop_targets, random_amplify\n",
    "from model.waveunet import Waveunet\n",
    "from test import evaluate, validate\n",
    "from os import environ as env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1058462-0a4f-481a-ba9c-93dff394c49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load env variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o -v ./config/config_train.yml\n",
    "# %reload_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b8991-653c-4f45-a9d6-168fe67a7c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # torch.backends.cudnn.benchmark=True # This makes dilated conv much faster for CuDNN 7.5\n",
    "\n",
    "    # MODEL\n",
    "    num_features = [args.features * i for i in range(1, args.levels + 1)] if args.feature_growth == \"add\" else \\\n",
    "        [args.features * 2 ** i for i in range(0, args.levels)]\n",
    "    target_outputs = int(args.output_size * args.sr)\n",
    "    model = Waveunet(args.channels, num_features, args.channels, args.instruments, kernel_size=args.kernel_size,\n",
    "                     target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                     conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
    "\n",
    "    if args.cuda:\n",
    "        model = model_utils.DataParallel(model)\n",
    "        print(\"move model to gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    print('model: ', model)\n",
    "    print('parameter count: ', str(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "    writer = SummaryWriter(args.log_dir)\n",
    "\n",
    "    ### DATASET\n",
    "    musdb = get_musdb_folds(args.dataset_dir)\n",
    "    # If not data augmentation, at least crop targets to fit model output shape\n",
    "    crop_func = partial(crop_targets, shapes=model.shapes)\n",
    "    # Data augmentation function for training\n",
    "    augment_func = partial(random_amplify, shapes=model.shapes, min=0.7, max=1.0)\n",
    "    train_data = SeparationDataset(musdb, \"train\", args.instruments, args.sr, args.channels, model.shapes, True,\n",
    "                                   args.hdf_dir, audio_transform=augment_func)\n",
    "    val_data = SeparationDataset(musdb, \"val\", args.instruments, args.sr, args.channels, model.shapes, False,\n",
    "                                 args.hdf_dir, audio_transform=crop_func)\n",
    "    test_data = SeparationDataset(musdb, \"test\", args.instruments, args.sr, args.channels, model.shapes, False,\n",
    "                                  args.hdf_dir, audio_transform=crop_func)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n",
    "                                             num_workers=args.num_workers, worker_init_fn=utils.worker_init_fn)\n",
    "\n",
    "    ##### TRAINING ####\n",
    "\n",
    "    # Set up the loss function\n",
    "    if args.loss == \"L1\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif args.loss == \"L2\":\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Couldn't find this loss!\")\n",
    "\n",
    "    # Set up optimiser\n",
    "    optimizer = Adam(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Set up training state dict that will also be saved into checkpoints\n",
    "    state = {\"step\": 0,\n",
    "             \"worse_epochs\": 0,\n",
    "             \"epochs\": 0,\n",
    "             \"best_loss\": np.Inf}\n",
    "\n",
    "    # LOAD MODEL CHECKPOINT IF DESIRED\n",
    "    if args.load_model is not None:\n",
    "        print(\"Continuing training full model from checkpoint \" + str(args.load_model))\n",
    "        state = model_utils.load_model(model, optimizer, args.load_model, args.cuda)\n",
    "\n",
    "    print('TRAINING START')\n",
    "\n",
    "    while state[\"worse_epochs\"] < args.patience:\n",
    "        print(\"Training one epoch from iteration \" + str(state[\"step\"]))\n",
    "        avg_time = 0.\n",
    "        model.train()\n",
    "        with tqdm(total=len(train_data) // args.batch_size) as pbar:\n",
    "            np.random.seed()\n",
    "            for example_num, (x, targets) in enumerate(dataloader):\n",
    "                if args.cuda:\n",
    "                    x = x.cuda()\n",
    "                    for k in list(targets.keys()):\n",
    "                        targets[k] = targets[k].cuda()\n",
    "\n",
    "                t = time.time()\n",
    "\n",
    "                # Set LR for this iteration\n",
    "                utils.set_cyclic_lr(optimizer, example_num, len(train_data) // args.batch_size, args.cycles,\n",
    "                                    args.min_lr, args.lr)\n",
    "                writer.add_scalar(\"lr\", utils.get_lr(optimizer), state[\"step\"])\n",
    "\n",
    "                # Compute loss for each instrument/model\n",
    "                optimizer.zero_grad()\n",
    "                outputs, avg_loss = model_utils.compute_loss(model, x, targets, criterion, compute_grad=True)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                t = time.time() - t\n",
    "                avg_time += (1. / float(example_num + 1)) * (t - avg_time)\n",
    "\n",
    "                writer.add_scalar(\"train_loss\", avg_loss, state[\"step\"])\n",
    "\n",
    "                if example_num % args.example_freq == 0 and args.save_audio_to_logs:\n",
    "                    input_centre = torch.mean(\n",
    "                        x[0, :, model.shapes[\"output_start_frame\"]:model.shapes[\"output_end_frame\"]],\n",
    "                        0)  # Stereo not supported for logs yet\n",
    "                    writer.add_audio(\"input\", input_centre, state[\"step\"], sample_rate=args.sr)\n",
    "\n",
    "                    for inst in outputs.keys():\n",
    "                        writer.add_audio(inst + \"_pred\", torch.mean(outputs[inst][0], 0), state[\"step\"],\n",
    "                                         sample_rate=args.sr)\n",
    "                        writer.add_audio(inst + \"_target\", torch.mean(targets[inst][0], 0), state[\"step\"],\n",
    "                                         sample_rate=args.sr)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # VALIDATE\n",
    "        val_loss = validate(args, model, criterion, val_data)\n",
    "        print(\"VALIDATION FINISHED: LOSS: \" + str(val_loss))\n",
    "        writer.add_scalar(\"val_loss\", val_loss, state[\"step\"])\n",
    "\n",
    "        # EARLY STOPPING CHECK\n",
    "        checkpoint_path = os.path.join(args.checkpoint_dir, \"checkpoint_\" + str(state[\"step\"]))\n",
    "        if val_loss >= state[\"best_loss\"]:\n",
    "            state[\"worse_epochs\"] += 1\n",
    "        else:\n",
    "            print(\"MODEL IMPROVED ON VALIDATION SET!\")\n",
    "            state[\"worse_epochs\"] = 0\n",
    "            state[\"best_loss\"] = val_loss\n",
    "            state[\"best_checkpoint\"] = checkpoint_path\n",
    "\n",
    "        state[\"epochs\"] += 1\n",
    "        if state[\"epochs\"] % 10 == 0:\n",
    "            # CHECKPOINT\n",
    "            print(\"Saving model...\")\n",
    "            model_utils.save_model(model, optimizer, state, checkpoint_path)\n",
    "\n",
    "    #### TESTING ####\n",
    "    # Test loss\n",
    "    print(\"TESTING\")\n",
    "\n",
    "    # Load best model based on validation loss\n",
    "    state = model_utils.load_model(model, None, state[\"best_checkpoint\"], args.cuda)\n",
    "    test_loss = validate(args, model, criterion, test_data)\n",
    "    print(\"TEST FINISHED: LOSS: \" + str(test_loss))\n",
    "    writer.add_scalar(\"test_loss\", test_loss, state[\"step\"])\n",
    "\n",
    "    # Mir_eval metrics\n",
    "    test_metrics = evaluate(args, musdb[\"test\"], model, args.instruments)\n",
    "\n",
    "    # Dump all metrics results into pickle file for later analysis if needed\n",
    "    with open(os.path.join(args.checkpoint_dir, \"results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(test_metrics, f)\n",
    "\n",
    "    # Write most important metrics into Tensorboard log\n",
    "    avg_SDRs = {inst: np.mean([np.nanmean(song[inst][\"SDR\"]) for song in test_metrics]) for inst in args.instruments}\n",
    "    avg_SIRs = {inst: np.mean([np.nanmean(song[inst][\"SIR\"]) for song in test_metrics]) for inst in args.instruments}\n",
    "    for inst in args.instruments:\n",
    "        writer.add_scalar(\"test_SDR_\" + inst, avg_SDRs[inst], state[\"step\"])\n",
    "        writer.add_scalar(\"test_SIR_\" + inst, avg_SIRs[inst], state[\"step\"])\n",
    "    overall_SDR = np.mean([v for v in avg_SDRs.values()])\n",
    "    writer.add_scalar(\"test_SDR\", overall_SDR)\n",
    "    print(\"SDR: \" + str(overall_SDR))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43a0ba-9114-4fc5-8905-1a895861aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--instruments', type=str, nargs='+', default=[\"other\", \"Ru-106\"],\n",
    "                    help=\"List of instruments to separate (default: \\\"Ru-106 Co-60 other\\\")\")\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='Use CUDA (default: False)')\n",
    "parser.add_argument('--features', type=int, default=int(env['FEATURES']),\n",
    "                    help='Number of feature channels per layer')\n",
    "parser.add_argument('--load_model', type=str, default=str(env['LOAD_MODEL']),\n",
    "                    help='Reload a previously trained model')\n",
    "parser.add_argument('--batch_size', type=int, default=int(env['BATCH_SIZE']),\n",
    "                    help=\"Batch size\")\n",
    "parser.add_argument('--levels', type=int, default=6,\n",
    "                    help=\"Number of DS/US blocks\")\n",
    "parser.add_argument('--depth', type=int, default=int(env['DEPTH']),\n",
    "                    help=\"Number of convs per block\")\n",
    "parser.add_argument('--sr', type=int, default=int(env['SR']), help=\"Sampling rate\")\n",
    "parser.add_argument('--channels', type=int, default=int(env['CHANNELS']), help=\"Number of input audio channels\")\n",
    "parser.add_argument('--kernel_size', type=int, default=int(env['KERNEL_SIZE']),\n",
    "                    help=\"Filter width of kernels. Has to be an odd number\")\n",
    "parser.add_argument('--output_size', type=float, default=float(env['OUTPUT_SIZE']),\n",
    "                    help=\"Output duration\")\n",
    "parser.add_argument('--strides', type=int, default=int(env['STRIDES']),\n",
    "                    help=\"Strides in Waveunet\")\n",
    "parser.add_argument('--conv_type', type=str, default=str(env['CONV_TYPE']),\n",
    "                    help=\"Type of convolution (normal, BN-normalised, GN-normalised): normal/bn/gn\")\n",
    "parser.add_argument('--res', type=str, default=str(env['RES']),\n",
    "                    help=\"Resampling strategy: fixed sinc-based lowpass filtering or learned conv layer: fixed/learned\")\n",
    "parser.add_argument('--separate', type=int, default=int(env['SEPARATE']),\n",
    "                    help=\"Train separate model for each source (1) or only one (0)\")\n",
    "parser.add_argument('--feature_growth', type=str, default=str(env['FEATURE_GROWTH']),\n",
    "                    help=\"How the features in each layer should grow, either (add) the initial number of features each time, or multiply by 2 (double)\")\n",
    "parser.add_argument('--input', type=str, default=str(env['INPUT']),\n",
    "                    help=\"Path to input mixture to be separated\")\n",
    "parser.add_argument('--output', type=str, default = None if env['OUTPUT'] == 'None' else env['OUTPUT'], help=\"Output path (same folder as input path if not set)\")\n",
    "args = parser.parse_args(\"--cuda\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d20539-ae61-4ec0-840e-38283091ca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main(args)\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ec213-6085-4a58-b51a-0f09138593d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
