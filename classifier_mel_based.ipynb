{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ignore-in-py"
    ]
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to html \"classifier_mel_based.ipynb\"\n",
    "\n",
    "# !jupyter nbconvert \\\n",
    "#      --TagRemovePreprocessor.enabled=True \\\n",
    "#      --TagRemovePreprocessor.remove_cell_tags=\"['ignore-in-py']\" \\\n",
    "#      --to script \"classifier_mel_based.ipynb\" --output \"./classifier_mel_based\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import math\n",
    "import os\n",
    "from random import choice\n",
    "\n",
    "from keras.models import load_model\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1681389787505,
     "user": {
      "displayName": "Alexey Arbuzov",
      "userId": "15402079048145436697"
     },
     "user_tz": -180
    },
    "id": "5-uw-ZaR1QX9"
   },
   "outputs": [],
   "source": [
    "def baseline_correction(raman_spectra, lam, p, niter=10):\n",
    "    i = 0\n",
    "    while raman_spectra[i] == 0:\n",
    "        i += 1\n",
    "\n",
    "    # according to \"Asymmetric Least Squares Smoothing\" by P. Eilers and H. Boelens\n",
    "    baseline_data = np.zeros(564 - i - 1)\n",
    "    curr_dataset = raman_spectra[i:]\n",
    "    # this is the code for the fitting procedure\n",
    "    L = len(curr_dataset)\n",
    "    w = np.ones(L)\n",
    "    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L - 2))\n",
    "    for jj in range(int(niter)):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = spsolve(Z, w * curr_dataset)\n",
    "        w = p * (curr_dataset > z) + (1 - p) * (curr_dataset < z)\n",
    "    # end of fitting procedure\n",
    "\n",
    "    baseline_data = z\n",
    "\n",
    "    baseline_data = np.concatenate((np.zeros(i), baseline_data))\n",
    "    return baseline_data\n",
    "\n",
    "\n",
    "# kr0, kr1 = 17.354178,2.883237\n",
    "ks0, ks1, ks2 = -1.627857, 1.235633, 0.0\n",
    "\n",
    "\n",
    "def channel_to_energy_channel(ch, ks0, ks1, ks2):\n",
    "    return ks0 + ks1 * ch + ks2 * ch ** (2)\n",
    "\n",
    "\n",
    "def calibrate_spectrum_by_energy(spectrum):\n",
    "    spectrum.rename(columns={0: 'channel', 1: 'counts'}, inplace=True)\n",
    "    counts_orig = spectrum['counts'].values\n",
    "    counts_new = np.zeros(int(channel_to_energy_channel(len(spectrum), ks0, ks1, ks2)) + 1)\n",
    "    for i in range(1, len(spectrum) + 1):\n",
    "        ch_ = channel_to_energy_channel(i, ks0, ks1, ks2)\n",
    "        if ch_ <= int(channel_to_energy_channel(len(spectrum), ks0, ks1, ks2)) + 1 and ch_ > 20:\n",
    "            K = counts_orig[i - 1]\n",
    "            xv = K * (ch_ // 1 - ch_ + 1)\n",
    "            counts_new[int(ch_ // 1) - 1] += xv\n",
    "            counts_new[int(ch_ // 1)] += K - xv\n",
    "    return counts_new\n",
    "\n",
    "\n",
    "# @numba.njit\n",
    "def drift_spectr(canals_orig, counts_orig, drift):\n",
    "    canals_drift = np.arange(1, max(int(len(canals_orig)) + 1, int(len(canals_orig) * (1 + drift) // 1) + 2))\n",
    "    counts_drift = np.zeros(max(int(len(canals_orig)), int(len(canals_orig) * (1 + drift) // 1) + 1))\n",
    "    for i in range(1, len(canals_orig)):\n",
    "        ch0 = canals_orig[i]\n",
    "        ch_ = ch0 * (1 + drift)\n",
    "        K = counts_orig[i]\n",
    "        x = K * (ch_ // 1 - ch_ + 1)\n",
    "        counts_drift[int(ch_ // 1) - 1] += x\n",
    "        counts_drift[int(ch_ // 1)] += K - x\n",
    "    if len(canals_drift) >= len(canals_orig):\n",
    "        canals_drift = canals_drift[:len(canals_orig)]\n",
    "        counts_drift = counts_drift[:len(canals_orig)]\n",
    "    return canals_drift, np.round(counts_drift, 0)\n",
    "\n",
    "\n",
    "def countNM_for_class(N, alfa, amount_in_class, min_amount_in_class):\n",
    "    A = min_amount_in_class * (1 + N + alfa * N ** 2) / amount_in_class - 1\n",
    "    N1 = (2 * alfa) ** (-1) * ((4 * alfa * A + 1) ** (.5) - 1)\n",
    "    M1 = alfa * N1\n",
    "    return int(round(N1, 0)), int(round(M1, 0))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def generate_spectr(num_channels, num_quants, spectr_list):\n",
    "    chnls = np.zeros(num_channels)\n",
    "    for i in range(num_quants):\n",
    "        target = random.random()\n",
    "        array = np.asarray(spectr_list)\n",
    "        idx = (np.abs(spectr_list - target)).argmin()\n",
    "        num = array[idx]\n",
    "        chnls[np.where(spectr_list == num)[0][-1]] += 1\n",
    "    return chnls\n",
    "\n",
    "\n",
    "def make_dataset1(main_dir_for_dataset):\n",
    "    fldrs = [f for f in os.listdir(main_dir_for_dataset) if not f.startswith('.')]\n",
    "    labels_dict = dict((k, v) for k, v in enumerate(fldrs, start=1))\n",
    "    num_energies = int(channel_to_energy_channel(2048, ks0, ks1, ks2)) + 1\n",
    "    data = np.empty([0, num_energies])\n",
    "    labels = []\n",
    "    for fldr in fldrs:\n",
    "        files = [f for f in os.listdir(f'{main_dir_for_dataset}/{fldr}') if not f.startswith('.')]\n",
    "        for fl in files:\n",
    "            ds = pd.read_csv(f'{main_dir_for_dataset}/{fldr}/{fl}', header=None)\n",
    "            ds = ds[ds[1].notna()]\n",
    "            calibrated_spectrum = calibrate_spectrum_by_energy(ds)\n",
    "            data = np.concatenate((data, calibrated_spectrum.reshape(1, num_energies)), axis=0)\n",
    "            labels.append(list(labels_dict.keys())[list(labels_dict.values()).index(fldr)])\n",
    "    datax = pd.DataFrame(data, columns=np.linspace(1, data.shape[1], data.shape[1]))\n",
    "    datax['label'] = labels\n",
    "    return datax, labels_dict\n",
    "\n",
    "    # N =  количество спектров, сгенеренных из 1го с аугментацией по вертикали (симуляция измерения во времени)\n",
    "    # N*alfa =  количество спектров, сгенеренных из 1го с аугментацией по горизонтали (симуляция сдвига по энергии)\n",
    "    # mu - средний дрифт\n",
    "    # sigma - дисперсия дрифта\n",
    "\n",
    "\n",
    "def make_augmentationed_dataset_from_dataset1(N, alfa, mu, sigma, data):\n",
    "    print('Begin: ', dt.datetime.now())\n",
    "    M = alfa * N\n",
    "    min_amount_in_class = data['label'].value_counts().min()\n",
    "    canals_orig = np.array(data.columns[:-1])\n",
    "    num_energies = int(channel_to_energy_channel(2048, ks0, ks1, ks2)) + 1\n",
    "    dataset = np.empty([0, num_energies + 1])\n",
    "    for i in range(len(data)):\n",
    "        x = 1\n",
    "\n",
    "        num_label = np.array(data.iloc[[i]])[0][-1]\n",
    "        amnt_in_class = data['label'].value_counts()[num_label]\n",
    "        N1, M1 = countNM_for_class(N, alfa, amnt_in_class, min_amount_in_class)\n",
    "        data1 = np.zeros((1 + N1 + N1 * M1, num_energies + 1))\n",
    "\n",
    "        data1[0] = np.array(data.iloc[[i]])[0]\n",
    "        counts_orig = np.array(data.iloc[[i]])[0][:-1]\n",
    "        for k in range(N1):\n",
    "            rq = sum(counts_orig)\n",
    "            n = int(random.random() * rq)\n",
    "            dfc = np.array(counts_orig.cumsum() / sum(counts_orig))\n",
    "            counts_new = generate_spectr(len(canals_orig), n, dfc)\n",
    "            data1[x][:-1] = counts_new\n",
    "            data1[x][-1] = num_label\n",
    "            x += 1\n",
    "\n",
    "            for j in range(M1):\n",
    "                drift = np.random.normal(mu, sigma, 1)[0]\n",
    "                canals_drift, counts_drift = drift_spectr(canals_orig, counts_new, drift)\n",
    "                data1[x][:-1] = counts_drift\n",
    "                data1[x][-1] = num_label\n",
    "                x += 1\n",
    "\n",
    "        dataset = np.concatenate((dataset, data1), axis=0)\n",
    "        nuclid = labels_dict[data.loc[i]['label']]\n",
    "        print(\n",
    "            f'Class number {num_label} nuclide {nuclid} done!\\nAdded {len(data1)} spectrums, dataset shape {dataset.shape}\\n ',\n",
    "            dt.datetime.now())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_dataset_spectrum_minus_baseline(rms1, len_ds):\n",
    "    ds_rms_bsln = np.zeros((len_ds, 2529))\n",
    "    for e in range(len_ds):\n",
    "        rms = rms1[e]\n",
    "        baseline_data = baseline_correction(rms, 1000, 0.0001, niter=10)\n",
    "        rms_bsln = np.array([rms[f] - baseline_data[f] if baseline_data[f] >= 0 else rms[f] for f in range(len(rms))])\n",
    "        rms_bsln[rms_bsln < 0] = 0\n",
    "        ds_rms_bsln[e] = rms_bsln\n",
    "        if e % 1000 == 0:\n",
    "            print(f'{e} from {len_ds} done! {round(e / len_ds * 100, 2)}% ', dt.datetime.now())\n",
    "    return ds_rms_bsln\n",
    "\n",
    "\n",
    "def mel_transform(params, E):\n",
    "    s = params[0]\n",
    "    a = params[4]\n",
    "    m = params[5]\n",
    "    b = params[3] + m * a\n",
    "    c = - params[1] / m\n",
    "    A = 1 / (b + a * m + 1 / m)\n",
    "    B = -A * a\n",
    "    C = A * c / m\n",
    "    x1 = E ** .5\n",
    "    return 2 * s * (B * math.log(abs(a * x1 ** 2 + b * x1 + c)) / (2 * a) + A * math.log(abs(x1 - m)) - (\n",
    "            B * b - 2 * C * a) * math.atan((2 * a * x1 + b) / (4 * a * c - b * b) ** .5) / (\n",
    "                            a * (4 * a * c - b * b) ** .5))\n",
    "\n",
    "\n",
    "def make_mel_scale(params, kevs_to_drop):\n",
    "    num_energies = int(channel_to_energy_channel(2048, ks0, ks1, ks2)) + 1\n",
    "    C2 = - mel_transform(params, kevs_to_drop)\n",
    "    xx = np.linspace(kevs_to_drop, num_energies, num_energies - kevs_to_drop + 1, dtype=int)\n",
    "    es = np.zeros(len(xx) + kevs_to_drop)\n",
    "    for E in xx:\n",
    "        es[E] = mel_transform(params, E) + C2\n",
    "    return es\n",
    "\n",
    "\n",
    "def generate_mel_spectrum(nfilt, es, spec):\n",
    "    low_mel = 0\n",
    "    high_mel = es[-1]\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)\n",
    "    mel_spectr = np.zeros(nfilt)\n",
    "    for i in range(1, nfilt + 1):\n",
    "        spectr_in_minus = np.where((es >= mel_points[i - 1]) & (es < mel_points[i]))\n",
    "        spectr_in_plus = np.where((es >= mel_points[i]) & (es < mel_points[i + 1]))\n",
    "        es_minus = es[spectr_in_minus]\n",
    "        es_plus = es[spectr_in_plus]\n",
    "        kefs_minus = np.zeros(len(spectr_in_minus[0]))\n",
    "        kefs_plus = np.zeros(len(spectr_in_plus[0]))\n",
    "        for g in range(len(spectr_in_minus[0])):\n",
    "            kefs_minus[g] = (es_minus[g] - mel_points[i - 1]) / (mel_points[i] - mel_points[i - 1])\n",
    "        for j in range(len(spectr_in_plus[0])):\n",
    "            kefs_plus[j] = (mel_points[i + 1] - es_plus[j]) / (mel_points[i + 1] - mel_points[i])\n",
    "        mel_spectr[i - 1] = np.dot(kefs_minus, spec[spectr_in_minus]) + np.dot(kefs_plus, spec[spectr_in_plus])\n",
    "    return mel_spectr\n",
    "\n",
    "\n",
    "def generate_mel_transformed_dataset(rms2):\n",
    "    mel_dataset = np.zeros((len(rms2), len(mel_points) - 2))\n",
    "    for i in range(len(rms2)):\n",
    "        spec = rms2[i]\n",
    "        mel_dataset[i] = generate_mel_spectrum(nfilt, es, spec)\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i} from {len(rms2)} done! {round(i / len(rms2) * 100, 2)}% ', dt.datetime.now())\n",
    "    return mel_dataset\n",
    "\n",
    "\n",
    "def make_mixed_spectrums_dataset(data, classes_amnt, num_sets):\n",
    "    data1 = data.values\n",
    "    mix_data1 = np.zeros((136 * num_sets, 2529))\n",
    "    mix_labels1 = [];\n",
    "    weights_comp = []\n",
    "    k = 0\n",
    "    for w in range(num_sets):\n",
    "        for i in range(1, classes_amnt + 1):\n",
    "            nomer_comp1 = choice(np.where(data1[:, -1] == i)[0])\n",
    "            comp1 = data1[nomer_comp1][:-1]\n",
    "            for j in range(i, classes_amnt + 1):\n",
    "                nomer_comp2 = choice(np.where(data1[:, -1] == j)[0])\n",
    "                comp2 = data1[nomer_comp2][:-1]\n",
    "                mix_data1[k] = comp1 + comp2\n",
    "                mix_labels1.append([i, j])\n",
    "                weights_comp.append([sum(comp1) / (sum(comp1) + sum(comp2)), sum(comp2) / (sum(comp1) + sum(comp2))])\n",
    "                k += 1\n",
    "    datax1 = pd.DataFrame(mix_data1, columns=np.linspace(1, mix_data1.shape[1], mix_data1.shape[1]))\n",
    "    datax1['label'] = mix_labels1\n",
    "    datax1['weights'] = weights_comp\n",
    "    return datax1\n",
    "\n",
    "\n",
    "def make_drifted_only_dataset(datax1, Num_drifts):\n",
    "    x = 0\n",
    "    data1 = np.zeros((len(datax1) * (1 + Num_drifts), num_energies))\n",
    "    mix_labels_list = [];\n",
    "    weights_list = []\n",
    "    canals_orig = np.array(datax1.columns[:-2])\n",
    "    for i in range(len(datax1)):\n",
    "        counts_orig = np.array(datax1.iloc[[i]])[0][:-2]\n",
    "        mix_label = datax1.iloc[[i]]['label'].values[0]\n",
    "        weights = datax1.iloc[[i]]['weights'].values[0]\n",
    "        data1[x] = counts_orig\n",
    "        mix_labels_list.append(mix_label)\n",
    "        weights_list.append(weights)\n",
    "        x += 1\n",
    "        for j in range(Num_drifts):\n",
    "            drift = np.random.normal(mu, sigma, 1)[0]\n",
    "            canals_drift, counts_drift = drift_spectr(canals_orig, counts_orig, drift)\n",
    "            data1[x] = counts_drift\n",
    "            mix_labels_list.append(mix_label)\n",
    "            weights_list.append(weights)\n",
    "            x += 1\n",
    "    dataset = pd.DataFrame(data1, columns=np.linspace(1, data1.shape[1], data1.shape[1]))\n",
    "    dataset['label'] = mix_labels_list\n",
    "    dataset['weights'] = weights_list\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def make_mixed_spectrums_dataset_from_number_of_nuclides(nclds_in_mix, num_mixes, datav):\n",
    "    mixes_vals = np.full((num_mixes, datav.shape[1] - 1), np.nan, dtype=np.float64)\n",
    "    label_ind_mixes = np.full((num_mixes, classes_amnt), 0)\n",
    "    for i in range(num_mixes):\n",
    "        rnd_indxs = random.choice(len(datav), nclds_in_mix)\n",
    "        mixes_vals[i] = np.sum(datav[rnd_indxs][:, :-1], axis=0)\n",
    "        list_label_inds = datav[rnd_indxs][:, -1]\n",
    "        for j in range(len(list_label_inds)):\n",
    "            label_ind_mixes[i][int(list_label_inds[j] - 1)] = 1\n",
    "    mixes_vals_df = pd.DataFrame(mixes_vals, columns=np.linspace(1, mixes_vals.shape[1], mixes_vals.shape[1]))\n",
    "    label_ind_mixes_df = pd.DataFrame(label_ind_mixes, columns=[labels_dict[k + 1] for k in range(classes_amnt)])\n",
    "    return mixes_vals_df, label_ind_mixes_df\n",
    "\n",
    "\n",
    "def make_drifts_of_mixes(mixes_vals, label_ind_mixes, Num_drifts):\n",
    "    x = 0\n",
    "    data1 = np.zeros((len(mixes_vals) * (1 + Num_drifts), num_energies))\n",
    "    labels_vektorS = np.zeros((len(mixes_vals) * (1 + Num_drifts), classes_amnt))\n",
    "    canals_orig = np.linspace(1, mixes_vals.shape[1], mixes_vals.shape[1])\n",
    "    for i in range(len(mixes_vals)):\n",
    "        counts_orig = mixes_vals[i]\n",
    "        labels_vektorS[x] = label_ind_mixes[i]\n",
    "        data1[x] = counts_orig\n",
    "        x += 1\n",
    "        for j in range(Num_drifts):\n",
    "            drift = np.random.normal(mu, sigma, 1)[0]\n",
    "            canals_drift, counts_drift = drift_spectr(canals_orig, counts_orig, drift)\n",
    "            data1[x] = counts_drift\n",
    "            labels_vektorS[x] = label_ind_mixes[i]\n",
    "            x += 1\n",
    "    data1_df = pd.DataFrame(data1, columns=np.linspace(1, mixes_vals.shape[1], mixes_vals.shape[1]))\n",
    "    labels_vektorS_df = pd.DataFrame(labels_vektorS, columns=[labels_dict[k + 1] for k in range(classes_amnt)])\n",
    "    df = pd.concat([data1_df, labels_vektorS_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_random_weights(n):\n",
    "    if n == 1:\n",
    "        return [1]\n",
    "    else:\n",
    "        zzz = np.sort(np.random.random(n - 1))\n",
    "        if n == 2:\n",
    "            return [zzz[0], 1 - zzz[0]]\n",
    "        else:\n",
    "            zzzr = [zzz[0]]\n",
    "            for i in range(len(zzz) - 1):\n",
    "                zzzr.append(zzz[i + 1] - zzz[i])\n",
    "            zzzr.append(1 - zzz[i + 1])\n",
    "            return zzzr\n",
    "\n",
    "\n",
    "def gen_smix(num_nclds_in_mix):\n",
    "    randm_ncld_clss_lst = np.random.choice(ncldsss_lst, num_nclds_in_mix, replace=False)\n",
    "    wt = generate_random_weights(num_nclds_in_mix)\n",
    "    smix0 = np.zeros(num_energies)\n",
    "    scnts0 = np.zeros(num_nclds_in_mix)\n",
    "    for i in range(num_nclds_in_mix):\n",
    "        smix = data_mono[data_mono['nuclide'] == randm_ncld_clss_lst[i]].values\n",
    "        smix = smix[int(np.random.choice(np.linspace(0, len(smix) - 1, len(smix))))][:-1]\n",
    "        smix0 = smix0 + smix * wt[i]\n",
    "        scnts0[i] = sum(smix * wt[i])\n",
    "    wtF = scnts0 / sum(scnts0)\n",
    "    return randm_ncld_clss_lst, wtF, smix0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIY1RoeWjYVm",
    "tags": [
     "ignore-in-py"
    ]
   },
   "source": [
    "\n",
    "## Чтение сохраненных моделей и их анализ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nfilt = int(141*4)\n",
    "q1,q2,q3,q4=-4.972648,3.076921,-0.050956,0.000696\n",
    "s = 10\n",
    "kevs_to_drop = 20\n",
    "x = symbols('x')\n",
    "m = solve(q1 + q2*x + q3*x**(2) + q4*x**(3), x)[0]\n",
    "params = np.array([s,q1,q2,q3,q4,m])\n",
    "es = make_mel_scale(params,kevs_to_drop)\n",
    "low_mel = 0\n",
    "high_mel = es[-1]\n",
    "mel_points = np.linspace(low_mel, high_mel, nfilt + 2)\n",
    "fig,ax = plt.subplots(figsize=(30,10))\n",
    "ax.plot(np.linspace(0,2529,2530),es)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_dir_for_dataset = './GammaSpectraGeant4/'\n",
    "\n",
    "N,alfa,mu,sigma = 5, 1, 0, 0.01\n",
    "Num_drifts = 4\n",
    "classes_amnt = 16\n",
    "num_sets = 33\n",
    "num_energies = int(channel_to_energy_channel(2048,ks0, ks1, ks2)) + 1\n",
    "data ,labels_dict= make_dataset1(main_dir_for_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mono = data.copy()\n",
    "data_mono['nuclide'] = [labels_dict[k] for k in data_mono['label'].values]\n",
    "data_mono.drop(columns='label',inplace=True)\n",
    "data_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncldsss_lst = np.unique(data_mono['nuclide'])\n",
    "ncldsss_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 101687,
     "status": "ok",
     "timestamp": 1681206538347,
     "user": {
      "displayName": "Родион Муравьев",
      "userId": "17125831210032811642"
     },
     "user_tz": -180
    },
    "id": "DQ8E9H1pIfGC",
    "outputId": "3176bea8-31a1-40eb-f88c-5feb06e4673b",
    "tags": [
     "ignore-in-py"
    ]
   },
   "outputs": [],
   "source": [
    "saved_model_Am_241 = load_model(f'./classification_models_mononuclides/best_model_Am-241.h5')\n",
    "saved_model_Ce_141 = load_model(f'./classification_models_mononuclides/best_model_Ce-141.h5')\n",
    "saved_model_Ce_144 = load_model(f'./classification_models_mononuclides/best_model_Ce-144.h5')\n",
    "saved_model_Co_58 = load_model(f'./classification_models_mononuclides/best_model_Co-58.h5')\n",
    "saved_model_Co_60 = load_model(f'./classification_models_mononuclides/best_model_Co-60.h5')\n",
    "saved_model_Cr_51 = load_model(f'./classification_models_mononuclides/best_model_Cr-51.h5')\n",
    "saved_model_Cs_134 = load_model(f'./classification_models_mononuclides/best_model_Cs-134.h5')\n",
    "saved_model_Cs_137 = load_model(f'./classification_models_mononuclides/best_model_Cs-137.h5')\n",
    "saved_model_Eu_152 = load_model(f'./classification_models_mononuclides/best_model_Eu-152.h5')\n",
    "saved_model_Fe_59 = load_model(f'./classification_models_mononuclides/best_model_Fe-59.h5')\n",
    "saved_model_I_131 = load_model(f'./classification_models_mononuclides/best_model_I-131.h5')\n",
    "saved_model_Mn_54 = load_model(f'./classification_models_mononuclides/best_model_Mn-54.h5')\n",
    "saved_model_Ru_103 = load_model(f'./classification_models_mononuclides/best_model_Ru-103.h5')\n",
    "saved_model_Ru_106 = load_model(f'./classification_models_mononuclides/best_model_Ru-106.h5')\n",
    "saved_model_Zn_65 = load_model(f'./classification_models_mononuclides/best_model_Zn-65.h5')\n",
    "saved_model_Zr_95 = load_model(f'./classification_models_mononuclides/best_model_Zr-95.h5')\n",
    "\n",
    "# saved_model_Am_241 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Am-241.h5')\n",
    "# saved_model_Ce_141 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Ce-141.h5')\n",
    "# saved_model_Ce_144 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Ce-144.h5')\n",
    "# saved_model_Co_58 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Co-58.h5')\n",
    "# saved_model_Co_60 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Co-60.h5')\n",
    "# saved_model_Cr_51 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Cr-51.h5')\n",
    "# saved_model_Cs_134 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Cs-134.h5')\n",
    "# saved_model_Cs_137 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Cs-137.h5')\n",
    "# saved_model_Eu_152 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Eu-152.h5')\n",
    "# saved_model_Fe_59 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Fe-59.h5')\n",
    "# saved_model_I_131 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_I-131.h5')\n",
    "# saved_model_Mn_54 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Mn-54.h5')\n",
    "# saved_model_Ru_103 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Ru-103.h5')\n",
    "# saved_model_Ru_106 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Ru-106.h5')\n",
    "# saved_model_Zn_65 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Zn-65.h5')\n",
    "# saved_model_Zr_95 = load_model(f'./classification_models_mononuclides/best_model_melBs_3mix_Zr-95.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncldsss_lst = np.unique(data_mono['nuclide'])\n",
    "comps,wesa,smix = gen_smix(2)\n",
    "\n",
    "weights_list = np.in1d(ncldsss_lst,comps).astype(float)\n",
    "w_true_list = np.zeros(len(ncldsss_lst))\n",
    "w_true_list[np.where(weights_list == 1)] = wesa\n",
    "\n",
    "drift = np.random.normal(mu, sigma, 1)[0]\n",
    "canals_orig = np.linspace(1,smix.shape[0],smix.shape[0])\n",
    "\n",
    "smes = drift_spectr(canals_orig, smix,drift)[1]\n",
    "#smes_log = np.log10(smes+1)/sum(np.log10(smes+1))\n",
    "mel_mix = generate_mel_spectrum(nfilt,es,smes)\n",
    "rmspec = np.log10(mel_mix + 1)\n",
    "# bsln = baseline_correction(rmspec,10,0.0001,niter=10)\n",
    "# bsln = np.where(bsln<0,0,bsln)\n",
    "# res = np.where((rmspec - bsln) < 0, 0, (rmspec - bsln))\n",
    "\n",
    "rms_bsln_o = rmspec/sum(rmspec)\n",
    "\n",
    "preds_list = np.zeros(len(ncldsss_lst))\n",
    "# preds_list[10] = saved_model_I_131.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[6] = saved_model_Cs_134.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[12] = saved_model_Ru_103.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[15] = saved_model_Zr_95.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[5] = saved_model_Cr_51.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "preds_list[0] = saved_model_Am_241.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "preds_list[4] = saved_model_Ru_106.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[1] = saved_model_Ce_141.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[2] = saved_model_Ce_144.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "preds_list[3] = saved_model_Eu_152.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "preds_list[2] = saved_model_Cs_137.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "preds_list[1] = saved_model_Co_60.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[11] = saved_model_Mn_54.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[3] = saved_model_Co_58.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[9] = saved_model_Fe_59.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "# preds_list[14] = saved_model_Zn_65.predict(rms_bsln_o.reshape((1,564)),batch_size = 1,verbose=0)[0][0]\n",
    "\n",
    "verificat_df = pd.DataFrame({'Nuclide':ncldsss_lst,'weight':weights_list,'true_weight':w_true_list,'predictions':preds_list}).T\n",
    "display(verificat_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1qxDn1KxqKdi0erw_05pDLrOTYm-wqWd3",
     "timestamp": 1681389660709
    },
    {
     "file_id": "155S_bb3viIoL0wAwkIyr1r8XQu4ARwA9",
     "timestamp": 1680253821228
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
